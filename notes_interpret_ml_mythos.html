<html><head><meta name="color-scheme" content="dark light"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>The Mythos of Model Interpretability</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: inherit;
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="187e0846-8e65-433b-8a31-e92cba227743" class="page sans"><header><h1 class="page-title">The Mythos of Model Interpretability</h1><table class="properties"><tbody><tr class="property-row property-row-created_time"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesCreatedAt"><path d="M6.98643729,14.0000972 C5.19579566,14.0000972 3.40419152,13.3106896 2.04245843,11.9323606 C-0.681017475,9.21200555 -0.680780251,4.76029539 2.04293482,2.04012507 C4.76664406,-0.68004331 9.22427509,-0.68004331 11.9480135,2.04013479 C13.272481,3.36277455 14,5.1330091 14,6.99552762 C14,8.87640182 13.2721894,10.6285043 11.9480135,11.9509302 C10.5679344,13.3105924 8.77756503,14.0000972 6.98643729,14.0000972 Z M10.2705296,7.00913883 L10.2705296,8.46099754 L10.2705296,8.65543362 L10.076181,8.65543362 L8.6543739,8.65543362 L5.72059514,8.65543362 L5.52619796,8.65543362 L5.52619796,8.46099754 L5.52619796,5.52541044 L5.52619796,3.37946773 L5.52619796,3.18502193 L5.72059514,3.18502193 L7.17253164,3.18502193 L7.36692883,3.18502193 L7.36692883,3.37946773 L7.36692883,6.81467358 L10.076181,6.81467358 L10.2705296,6.81467358 L10.2705296,7.00913883 Z M12.1601539,6.99552762 C12.1601539,5.61697497 11.6190112,4.32597154 10.6393933,3.34769528 C8.63253764,1.34336744 5.35197452,1.34061603 3.34153136,3.33944106 C3.33868273,3.34219247 3.33607716,3.34494388 3.33322852,3.34769528 C1.32397148,5.35459953 1.32372842,8.63641682 3.33322852,10.6433794 C5.34295224,12.6504489 8.62968901,12.6504489 10.6393933,10.6433794 C11.6190112,9.66506426 12.1601539,8.37408027 12.1601539,6.99552762 Z"></path></svg></span>Created</th><td><time>@Sep 28, 2020 7:19 PM</time></td></tr><tr class="property-row property-row-multi_select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesMultipleSelect"><path d="M4,3 C4,2.447715 4.447715,2 5,2 L12,2 C12.5523,2 13,2.447716 13,3 C13,3.55228 12.5523,4 12,4 L5,4 C4.447715,4 4,3.55228 4,3 Z M4,7 C4,6.447715 4.447715,6 5,6 L12,6 C12.5523,6 13,6.447716 13,7 C13,7.55228 12.5523,8 12,8 L5,8 C4.447715,8 4,7.55228 4,7 Z M4,11 C4,10.447715 4.447715,10 5,10 L12,10 C12.5523,10 13,10.447716 13,11 C13,11.55228 12.5523,12 12,12 L5,12 C4.447715,12 4,11.55228 4,11 Z M2,4 C1.44771525,4 1,3.55228475 1,3 C1,2.44771525 1.44771525,2 2,2 C2.55228475,2 3,2.44771525 3,3 C3,3.55228475 2.55228475,4 2,4 Z M2,8 C1.44771525,8 1,7.55228475 1,7 C1,6.44771525 1.44771525,6 2,6 C2.55228475,6 3,6.44771525 3,7 C3,7.55228475 2.55228475,8 2,8 Z M2,12 C1.44771525,12 1,11.5522847 1,11 C1,10.4477153 1.44771525,10 2,10 C2.55228475,10 3,10.4477153 3,11 C3,11.5522847 2.55228475,12 2,12 Z"></path></svg></span>Tags</th><td><span class="selected-value select-value-color-brown">Interpretable ML</span><span class="selected-value select-value-color-pink">Research</span></td></tr></tbody></table></header><div class="page-body"><p id="457dacc0-9560-485d-a944-2bf803e09bfe" class="">This paper :</p><ul id="dfa5a815-ce6e-4d17-9e39-4dfa4fa0a9c0" class="bulleted-list"><li>Tackles grounding some of the assumptions, definitions around Interpretable Machine Learning.</li></ul><ul id="ed6d07ee-48b9-4a38-807d-d3bc6f8f9494" class="bulleted-list"><li>Motivations behind Interpretability</li></ul><ul id="10c9ad95-05a0-4943-9a5f-a7e6b3dfdd6d" class="bulleted-list"><li>Identification of techniques and properties that make a model <em>interpretable.</em></li></ul><ul id="b89a1613-64c6-4380-8353-848d9998b1c6" class="bulleted-list"><li>Feasibility and Desirability of different notions</li></ul><ul id="d1bda2c7-f7a1-4d93-9c36-91633643ebd3" class="bulleted-list"><li>Questions the assumption &quot;Linear models are more interpretable and that DNNs are not&quot;</li></ul><p id="ee341e03-5b2d-4525-a0ee-00ff13ab412e" class="">
</p><h1 id="bb6d873b-566b-4936-b838-ab0d37913014" class="">Introduction</h1><ul id="57ce7f88-a618-4dfb-82ca-b803e893c6d9" class="bulleted-list"><li>Literature suggests that definitions of interpretability are ill-defined and claims, technical descriptions are diverse and sometimes not on the same page.</li></ul><ul id="61482067-8a3a-4ec3-9a51-9293c23ecccd" class="bulleted-list"><li>Papers propose Interpretability as a means to engender trust or identify causal connections in data.</li></ul><ul id="3fd4de00-530f-413e-a4dc-9bdc4fac9682" class="bulleted-list"><li>Models are trained on simplified objectives (metrics) which fail to capture the complexity of real life goals.</li></ul><ul id="b94c955d-18e6-48fb-8daf-20dcd2313a43" class="bulleted-list"><li>Divergence between real life and offline experimentation may also occur when there&#x27;s a distributional shift between the data shown to a supervised learner and the data it sees in the deployed environment. E.g. Recommendation, envs where current actions alter future timesteps.</li></ul><ul id="7a4dafbc-bf4e-4c2c-96f5-a864c62a0815" class="bulleted-list"><li>Often suggested that human decision-making is interpretable. But is that really true? i.e., humans cant really explain the mechanism by which brains work. T<strong>his suggests that often we look for useful information rather than exact mechanisms (Relevance Information :))</strong></li></ul><ul id="8751e380-5aa3-4783-b2f6-8da0b4bb823c" class="bulleted-list"><li>When are models interpretable? —&gt; We understand how the models actually work i.e., they are transparent. Do we look at parameters? Do we look at properties of the algorithm itself? Model Complexity (Is it even possible for a human to understand the model)?</li></ul><ul id="1fc88d0a-1210-46e2-9870-106cfb3d6e90" class="bulleted-list"><li>Post-hoc Interpretations are a class of methods which explain predictions without understanding the core of how the algorithms work. E.g. how we explain our thinking :)</li></ul><ul id="6399cf77-ceb0-4944-8a8d-7674c874a758" class="bulleted-list"><li><strong>Irony</strong>: Human brains are black boxes, but post-hoc interpretability is useful nevertheless. We still look to identify and demystify models to their core as much as possible.</li></ul><h1 id="7b1c1bea-ebfa-4c22-91ea-54e28125b103" class="">Desiderata of Interpretability Research</h1><p id="bc6435d5-ab2f-4e77-9a96-f02f073fb6d1" class="">Interpretability arises when there is a mismatch between the goals we set to out to solve and what an algorithm is predicting. Real World Objectives are often hard to encode in simple metrics such as test set predictive performance. E.g. Ethics and Legality.</p><h2 id="d682a116-a845-4877-97a2-0bae8f782500" class="">Trust</h2><ul id="05fad230-5576-49cc-8d7c-1c90d66000f8" class="bulleted-list"><li>Argument is that when a model is trustworthy, it is interpretable. How do we encode trust though?</li></ul><ul id="6173b5c5-b775-4227-859d-af0d89c555fb" class="bulleted-list"><li>Trust could be basically that confidence that the model will perform well with respect to the real world objectives.</li></ul><ul id="168462fa-5c88-4232-b201-10d854e3c6b5" class="bulleted-list"><li>Perhaps, it could also be said that we trust a model when we say that we are ok with it taking decisions.</li></ul><ul id="0e0512fd-d52f-4360-890a-8ad9fc21d6e2" class="bulleted-list"><li>We care not only about how many times it is right but also for which examples it is right. For e.g. if a model makes the same mistakes as a human, it could still potentially be called trustworthy by the human making mistakes because there&#x27;s essentially no cost of giving the model full control.</li></ul><h2 id="ef7ba50f-bdb0-4afc-b527-eb557bb51f4e" class="">Causality</h2><p id="f22aff82-bd0d-4c5c-a4f9-7e1c60857b88" class="">Potential identify causal relationships in supervised models. Supervised model are trained with goal of prediction by just association. Identification of causal relationships is not an ingredient but if possible, it helps to further scientists in buttoning down to the important feature-output relationships.</p><h2 id="e7de5b14-3902-4cb8-b37f-cc70d7a1e3cc" class="">Transferability</h2><ul id="2b04b1b5-fd88-4530-846b-410233805de8" class="bulleted-list"><li>Basically the argument is that under new conditions how does our model perform? Sometimes newer conditions invalidate any future predictions by the model e.g. asthma patients contracting Pneumonia. Model might predict lesser aggressive treatments for such patients if the model is trained to predict if a person will die if they have had asthma previously. This invalidates the model prediction.</li></ul><ul id="6ac8902c-6193-4a46-98aa-e9f33b2eb8d7" class="bulleted-list"><li>Adversarial Environments? E.g. security.</li></ul><ul id="c692deec-c530-4ffa-a38e-8dcf7f36df3d" class="bulleted-list"><li>Shift from IID data to OOD :)</li></ul><h2 id="2de8bd98-2498-4545-8372-813516b4642f" class="">Informativeness</h2><h2 id="3148c31c-2ee5-426c-9cad-3766ed8ac65d" class="">Fair and Ethical Decision Making</h2><h1 id="4872f955-7a05-4cdc-9ac8-9d8f7d4fb32a" class="">Properties of Interpretable Models</h1><h2 id="2b04c3e7-5849-4026-9ce9-3b2bca841647" class="">Transparency</h2><h3 id="58ca8f3a-2693-445e-b5df-b53ef02108ab" class="">Simulatability</h3><ul id="b6f72bc8-62c5-44d0-b2b4-25223a5d2d43" class="bulleted-list"><li>A model is transparent if a person can contemplate the entire model at once.</li></ul><ul id="a454a65d-e71f-43c3-b6db-78b1fa99e61b" class="bulleted-list"><li>When a human can take the input data together with the parameters of the model and produce a prediction in <em>reasonable</em> time by stepping through every calculation.</li></ul><ul id="deda3d42-56c3-4050-bed8-2e3153f65959" class="bulleted-list"><li>The definition of <em>reasonable</em> cannot be fixed as it depends on :<ul id="3c487e26-7069-4873-a9c4-08cb0d33e5b7" class="bulleted-list"><li>Model Size</li></ul><ul id="44c68937-21d3-4c8d-b433-cf10d8a25ce3" class="bulleted-list"><li>Computation required for inference. (Number of calculations)</li></ul></li></ul><ul id="d0466d6e-673f-41a0-a287-7414168c4b46" class="bulleted-list"><li>Finally, to say that &quot;linear or xyz model is intrinsically intepretable&quot; is wrong. Its just a relative term and its more appropriate to say that the simpler model is more transparent as compared to a high dimensional model.</li></ul><h2 id="039d889c-b21d-4d2b-b2ae-5d7b64c4c3d1" class="">Decomposability</h2><ul id="64feb5e8-3957-4141-a733-e53345e38fda" class="bulleted-list"><li>Each Input, param, and calculation offers an intuitive explanation as to why its predicting what it is. E.g. In decision trees nodes, weights in an NN can be thought of as the strength of an association between each feature and label.</li></ul><ul id="e85bf489-6f77-4778-b844-eab29afe0aa5" class="bulleted-list"><li>This notion requires input features themselves to be interpretable.</li></ul><h3 id="e65fee2e-b7a9-454f-a12c-842f1ed3e452" class="">Algorithmic Transparency</h3><ul id="ebe21e82-d8fe-4c03-94ca-bf7e2bc66833" class="bulleted-list"><li>Do we understand the shape of error surface?</li></ul><ul id="e04681a2-d4a4-4098-be91-682b9a8f42c5" class="bulleted-list"><li>Can we prove the algorithm to converge to the optimal solution?</li></ul><ul id="5351e382-627e-42fc-8d3a-4bf2c30cb869" class="bulleted-list"><li>Deep Learning methods lack algorithmic transparency. </li></ul><ul id="c46edc12-2596-4586-bf11-b4260932d079" class="bulleted-list"><li>Humans do not exhibit these forms of transparency.</li></ul><h2 id="303f457e-21e9-4cce-be45-dc885d6e64d8" class="">Post-hoc Interpretability</h2><p id="ef378367-c2b7-4673-9388-8a5336b17572" class="">Extraction of information from learned models.</p><p id="12e3a2d1-4a05-46c4-9e80-a8a444da186c" class="">In the form of :</p><ul id="f5555472-46c9-403c-b25a-86ec32364740" class="bulleted-list"><li>Natural Language Explanations, Visualizations of learned representations or models</li></ul><h3 id="c8b06779-fee0-4697-b228-b96e6063f6bc" class="">Text Explanations</h3><p id="92ea8787-52d7-43bd-89ef-adbf3ac8d7b0" class="">Methods</p><ul id="8125e890-81cd-4a5c-9f8b-73e7c626d263" class="bulleted-list"><li>Train an RNN in parallel to generate an explanation. Maximize likelihood of text.</li></ul><ul id="38537186-c278-4de0-8538-34f3c286456e" class="bulleted-list"><li>Neural Captioning is a perfect example.</li></ul><h3 id="58ebbc48-277e-4f11-af53-a960bfbd078d" class="">Visualization</h3><ul id="1da704c6-ffed-43f4-887e-5f7d76ce0acf" class="bulleted-list"><li>Use t-SNE to visualize high dimensional embeddings.</li></ul><ul id="4199e3ee-6059-49b7-97d6-4e0c7e03cdc0" class="bulleted-list"><li>Visualize filters in CNNs, etc</li></ul><h3 id="813f2ee3-8c84-45c9-bbf2-690217cdce6f" class="">Local Explanations</h3><p id="1d9d55de-d896-402d-b769-7512742a19b0" class="">Not global explanations, but for e.g. wrt a particular class.</p><h1 id="0e6872ff-3f58-4d95-955e-9bdc835b2d11" class="">Discussion</h1><h2 id="8c80f543-113b-418b-9574-c972cdc7f04e" class="">Linear models are not strictly more interpretable than DNNs</h2><p id="13c2e10e-ee79-4202-a62b-3d046c22d57a" class="">Linear Models - Algorithmic Transparency ✅</p><p id="cb82361a-c40b-465a-93d3-bd5a99940803" class="">Linear Models - Simulatibility ❌ (High Dimensional)</p><p id="fd273259-e03b-4f33-9f2b-08764a3e13fa" class="">Linear Models - Decomposability ❌ (Heavily Engineered features)</p><p id="85c8ab79-bdb9-4628-8d58-0e0a511b3e93" class="">There&#x27;s a tradeoff between algorithmic transparency vs decomposability when choosing between DNNs and Linear models. Atleast Post-hoc reasoning makes sense.</p><p id="f37c5f2d-6b6e-4aed-97d3-cba4764c2bc3" class="">
</p><p id="e4882469-0ec2-460e-9165-9f0a26fcb71e" class="">
</p></div></article></body></html>